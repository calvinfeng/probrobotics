{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive State Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts in Probability\n",
    "\n",
    "Let $X$ denotes a random variable, then $x$ is a specific value that $X$ might assume.\n",
    "\n",
    "$$\n",
    "p(X = x) \\;\\text{denotes the probability of $X$ has the value of $x$}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\sum_{x} p(X = x) = 1\n",
    "$$\n",
    "\n",
    "All continuous random variables possess probability density function, **PDF**.\n",
    "\n",
    "$$\n",
    "p(x) = (2\\pi\\sigma^{2})^{-1/2} \\exp{\\frac{-1}{2}\\frac{(x - \\mu)^{2}}{\\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "We can abbreviate the equation as follows, because it is a normal distribution.\n",
    "\n",
    "$$\n",
    "N(x; \\mu; \\sigma^{2})\n",
    "$$\n",
    "\n",
    "However, in general, $x$ is not a scalar value, it is generally a vector. Let $\\Sigma$ be a positive semidefinite and symmetric matrix, which is a **covariance matrix**.\n",
    "\n",
    "$$\n",
    "p(x) = det(2\\pi\\Sigma)^{-1/2} \\exp{ \\frac{-1}{2} (\\vec{x} - \\vec{\\mu})^{T} \\Sigma^{-1} (\\vec{x} - \\vec{\\mu})}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\int p(x) dx = 1\n",
    "$$\n",
    "\n",
    "The joint distribution of two random variables $X$ and $Y$ can be described as folows.\n",
    "\n",
    "$$\n",
    "p(x, y) = p(\\text{$X = x$ and $Y = y$})\n",
    "$$\n",
    "\n",
    "If they are independent, then\n",
    "\n",
    "$$\n",
    "p(x,y) = p(x)p(y)\n",
    "$$\n",
    "\n",
    "If they are conditioned, then\n",
    "\n",
    "$$\n",
    "p(x \\mid y) = p(X=x \\mid Y=y)\n",
    "$$\n",
    "\n",
    "If $p(y) > 0$, then\n",
    "\n",
    "$$\n",
    "p(x \\mid y) = \\frac{p(x, y)}{p(y)}\n",
    "$$\n",
    "\n",
    "**Theorem of Total Probability** states the following.\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{y} p(x \\mid y)p(y) = \\int p(x \\mid y)p(y)dy\n",
    "$$\n",
    "\n",
    "We can apply **Bayes Rule**. \n",
    "\n",
    "$$\n",
    "p(x \\mid y) = \\frac{ p(y \\mid x) p(x) }{ p(y) } = \\frac{ p(y \\mid x) p(x) } { \\sum_{x`} p(y \\mid x`) p(x`)}\n",
    "$$\n",
    "\n",
    "In integral form,\n",
    "\n",
    "$$\n",
    "\\frac{ p(y \\mid x) p(x) } { \\int p(y \\mid x`) p(x`) dx`}\n",
    "$$\n",
    "\n",
    "If $x$ is a quantity that we would like to inrefer from $y$, the probability $p(x)$ is referred as **prior probability distribution** and $y$ is called data, e.g. laser measurements. $p(x \\mid y)$ is called **posterior probability distribution** over $X$.\n",
    "\n",
    "In robotics, $p(y \\mid x)$ is called **generative model**. Since $p(y)$ does not depend on $x$, $p(y)^{-1}$ is often written as a normalizer in Bayes rule variables.\n",
    "\n",
    "$$\n",
    "p(x \\mid y) = \\eta p(y \\mid x) p(x)\n",
    "$$\n",
    "\n",
    "It is perfectly fine to to condition any of the rules on arbitrary random variables, e.g. the location of a robot can inferred from multiple sources of random measurements.\n",
    "\n",
    "$$\n",
    "p(x \\mid y, z) = \\frac{ p(x \\mid x, z) p(y \\mid z) }{ p(y \\mid z) }\n",
    "$$\n",
    "\n",
    "for as long as $p(y \\mid z) > 0$.\n",
    "\n",
    "Similarly, we can condition the rule for combining probabilities of independent random variables on other variables.\n",
    "\n",
    "$$\n",
    "p(x, y \\mid z) = p(x \\mid z)p(y \\mid z)\n",
    "$$\n",
    "\n",
    "However, conditional independence does not imply absolute indenpendence, that is\n",
    "\n",
    "$$\n",
    "p(x, y \\mid z) = p(x \\mid z)p(y \\mid z) \\neq p(x,y) = p(x)p(y)\n",
    "$$\n",
    "\n",
    "The converse is neither true, absolute independence does not imply conditional independence.\n",
    "\n",
    "The expected value of a random variable is given by\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_{x} x p(x) = \\int x p(x) dx\n",
    "$$\n",
    "\n",
    "Expectation is a linear function of a random variable, we have the following property.\n",
    "\n",
    "$$\n",
    "E[aX + b] = aE[X] + b\n",
    "$$\n",
    "\n",
    "Covariance measures the squared expected deviation from the mean. Therefore, square root of covariance is in fact variance, i.e. the expected deviation from the mean.\n",
    "\n",
    "$$\n",
    "Cov[X] = E[X - E[X]^{2} = E[X^{2}] - E[X]^2\n",
    "$$\n",
    "\n",
    "Finally, **entropy** of a probability distribution is given by the following expression. Entropy is the expected information that the value of $x$ carries. \n",
    "\n",
    "$$\n",
    "H_{p}(x) = -\\sum_{x} p(x) log_{2}p(x) = -\\int p(x) log_{2} p(x) dx\n",
    "$$\n",
    "\n",
    "In the discrete case, the $-log_{2}p(x)$ is the number of bits required to encode x using an optimal encoding, assuming that $p(x)$ is the probability of observing $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
